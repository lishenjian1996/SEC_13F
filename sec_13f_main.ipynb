{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 4 ZIP files and start to download and process.\n",
      "[ERROR] Invalid year range. Please enter a valid range between 2013 and 2030.\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "import magic  # pip install python-magic-bin\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set folders for storing data\n",
    "BASE_FOLDER = 'sec_13f_data'\n",
    "ANNUAL_FOLDER = 'sec_13f_annual'\n",
    "os.makedirs(BASE_FOLDER, exist_ok=True)\n",
    "os.makedirs(ANNUAL_FOLDER, exist_ok=True)\n",
    "\n",
    "# Step 1: Get SEC ZIP Links\n",
    "def get_zip_links(start_year, end_year):\n",
    "    url = \"https://www.sec.gov/data-research/sec-markets-data/form-13f-data-sets\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"MyAppName/1.0 (myemail@example.com)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.sec.gov/\",\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        zip_links = []\n",
    "        \n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if year >= 2024:\n",
    "                    if href.endswith('.zip') and re.search(rf\"{year}\", href):\n",
    "                        zip_links.append(f\"https://www.sec.gov{href}\")\n",
    "                else:\n",
    "                    if href.endswith('.zip') and f\"{year}q\" in href:\n",
    "                        zip_links.append(f\"https://www.sec.gov{href}\")\n",
    "        \n",
    "        if not zip_links:\n",
    "            raise Exception(\"[ERROR] No matching ZIP files found.\")\n",
    "        print(f\"[INFO] Found {len(zip_links)} ZIP files and start to download and process.\")\n",
    "        return zip_links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"[ERROR] Failed to fetch ZIP links: {e}\")\n",
    "\n",
    "# Step 2: Download ZIP Files\n",
    "def is_valid_zip(file_path):\n",
    "    mime_type = magic.Magic(mime=True).from_file(file_path)\n",
    "    return mime_type == 'application/zip'\n",
    "\n",
    "def download_zip(zip_url, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    local_zip = os.path.join(output_folder, os.path.basename(zip_url))\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"MyAppName/1.0 (myemail@example.com)\"\n",
    "    }\n",
    "    response = requests.get(zip_url, headers=headers, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(local_zip, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    \n",
    "    if not is_valid_zip(local_zip):\n",
    "        os.remove(local_zip)\n",
    "        raise Exception(\"[ERROR] Downloaded file is not a valid ZIP.\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    return local_zip\n",
    "\n",
    "# Step 3: Extract ZIP Files\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    try:\n",
    "        with ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "    except BadZipFile:\n",
    "        raise Exception(\"[ERROR] Invalid ZIP file.\")\n",
    "\n",
    "# Step 4: Process and Parsing TSV and convert to CSV\n",
    "def process_tsv_files(folder, output_csv):\n",
    "    infotable_df = None\n",
    "    \n",
    "    # Load INFOTABLE as the base\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.tsv'):\n",
    "            tsv_path = os.path.join(folder, file)\n",
    "            try:\n",
    "                df = pd.read_csv(tsv_path, sep='\\t', encoding='utf-8', engine='python')\n",
    "                if 'INFOTABLE' in file:\n",
    "                    infotable_df = df.copy()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to process {file}: {e}\")\n",
    "    \n",
    "    if infotable_df is None:\n",
    "        raise ValueError(\"[ERROR] No INFOTABLE file found.\")\n",
    "    \n",
    "    # Extract CIK\n",
    "    if 'ACCESSION_NUMBER' in infotable_df.columns:\n",
    "        infotable_df['CIK'] = infotable_df['ACCESSION_NUMBER'].str.split('-').str[0]\n",
    "    else:\n",
    "        raise ValueError(\"[ERROR] ACCESSION_NUMBER column is missing.\")\n",
    "    \n",
    "    # Merge Other TSV Files into INFOTABLE\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.tsv') and 'INFOTABLE' not in file:\n",
    "            tsv_path = os.path.join(folder, file)\n",
    "            try:\n",
    "                df = pd.read_csv(tsv_path, sep='\\t', encoding='utf-8', engine='python')\n",
    "                if 'ACCESSION_NUMBER' in df.columns:\n",
    "                    common_columns = set(infotable_df.columns).intersection(df.columns)\n",
    "                    df = df.drop(columns=[col for col in common_columns if col != 'ACCESSION_NUMBER'])\n",
    "                    infotable_df = pd.merge(\n",
    "                        infotable_df, df, on='ACCESSION_NUMBER', how='left'\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to merge {file}: {e}\")\n",
    "    \n",
    "    # Remove Duplicate Rows Based on Critical Subset\n",
    "    subset_columns = [\n",
    "        'CIK', 'FILINGMANAGER_NAME', 'SUBMISSIONTYPE', 'PERIODOFREPORT', 'FILING_DATE',\n",
    "        'NAMEOFISSUER', 'TITLEOFCLASS', 'CUSIP', 'VALUE', 'SSHPRNAMT'\n",
    "    ]\n",
    "    subset_columns = [col for col in subset_columns if col in infotable_df.columns]\n",
    "    before_dedup = len(infotable_df)\n",
    "    infotable_df.drop_duplicates(subset=subset_columns, inplace=True)\n",
    "    after_dedup = len(infotable_df)\n",
    "    \n",
    "    # Final Column Mapping\n",
    "    columns_to_keep = {\n",
    "        'CIK': 'cik',\n",
    "        'FILINGMANAGER_NAME': 'coname',\n",
    "        'SUBMISSIONTYPE': 'form',\n",
    "        'PERIODOFREPORT': 'rdate',\n",
    "        'FILING_DATE': 'fdate',\n",
    "        'NAMEOFISSUER': 'nameOfIssuer',\n",
    "        'TITLEOFCLASS': 'titleOfClass',\n",
    "        'CUSIP': 'cusip',\n",
    "        'VALUE': 'value',\n",
    "        'SSHPRNAMT': 'sshPrnamt',\n",
    "        'SSHPRNAMTTYPE': 'sshPrnamtType',\n",
    "        'PUTCALL': 'putCall',\n",
    "        'INVESTMENTDISCRETION': 'investmentDiscretion',\n",
    "        'OTHERMANAGER': 'otherManager',\n",
    "        'VOTING_AUTH_SOLE': 'Sole',\n",
    "        'VOTING_AUTH_SHARED': 'Shared',\n",
    "        'VOTING_AUTH_NONE': 'None'\n",
    "    }\n",
    "    \n",
    "    # Create final_df with selected columns\n",
    "    final_df = infotable_df[[col for col in columns_to_keep.keys() if col in infotable_df.columns]].copy()\n",
    "    final_df.rename(columns=columns_to_keep, inplace=True)\n",
    "    \n",
    "    # Save the final CSV\n",
    "    final_df.to_csv(output_csv, index=False)\n",
    "    print(f\"[INFO] Saved 13F CSV: {output_csv}\")\n",
    "    \n",
    "    # Post-process date columns in the saved CSV\n",
    "    postprocess_csv_dates(output_csv)\n",
    "    \n",
    "    # Cleanup temporary files\n",
    "    shutil.rmtree(folder)\n",
    "\n",
    "# Step 5: Main Workflow\n",
    "def main():\n",
    "    start_year = int(input(\"Enter start year: \"))\n",
    "    end_year = int(input(\"Enter end year: \"))\n",
    "    zip_links = get_zip_links(start_year, end_year)\n",
    "    \n",
    "    # Ensure the user enters a valid range\n",
    "    if start_year < 2013 or end_year > 2030 or start_year > end_year:\n",
    "        print(\"[ERROR] Invalid year range. Please enter a valid range between 2013 and 2030.\")\n",
    "        return\n",
    "    \n",
    "    for zip_link in zip_links:\n",
    "        zip_folder = os.path.join(BASE_FOLDER, os.path.basename(zip_link).replace('.zip', ''))\n",
    "        zip_path = download_zip(zip_link, zip_folder)\n",
    "        extract_zip(zip_path, zip_folder)\n",
    "        process_tsv_files(zip_folder, os.path.join(BASE_FOLDER, f\"{os.path.basename(zip_link).replace('.zip', '')}.csv\"))\n",
    "    \n",
    "    # Perform annual aggregation after processing all CSVs\n",
    "    aggregate_csv_by_year()\n",
    "    # Sort every csv file by rdate and cik\n",
    "    sort_csv_files()\n",
    "\n",
    "# Step 6: Post-Process Dates in Final CSV\n",
    "def postprocess_csv_dates(csv_file):\n",
    "    \"\"\"\n",
    "    Ensures date columns (rdate and fdate) are formatted as YYYY-MM-DD after CSV export.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Explicitly define column types to handle mixed data types\n",
    "        dtype_mapping = {\n",
    "            'rdate': 'str',\n",
    "            'fdate': 'str'\n",
    "        }\n",
    "        \n",
    "        df = pd.read_csv(csv_file, dtype=dtype_mapping, low_memory=False)\n",
    "        \n",
    "        # Explicitly format date columns\n",
    "        for date_col in ['rdate', 'fdate']:\n",
    "            if date_col in df.columns:\n",
    "                df[date_col] = pd.to_datetime(\n",
    "                    df[date_col], format='%d-%b-%Y', errors='coerce'\n",
    "                ).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Save the CSV again with updated date formats\n",
    "        df.to_csv(csv_file, index=False)    \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to format dates in final CSV: {e}\")\n",
    "\n",
    "# Step 7: Aggregate Final CSVs by Year Based on fdate\n",
    "def aggregate_csv_by_year():\n",
    "    \"\"\"\n",
    "    Aggregate all final CSV files by year based on the 'fdate' column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_files = [os.path.join(BASE_FOLDER, f) for f in os.listdir(BASE_FOLDER) if f.endswith('.csv')]\n",
    "        annual_data = {}\n",
    "        \n",
    "        for file in all_files:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            \n",
    "            # Ensure 'fdate' is in the correct datetime format\n",
    "            if 'fdate' in df.columns:\n",
    "                df['fdate'] = pd.to_datetime(df['fdate'], errors='coerce')\n",
    "                df['year'] = df['fdate'].dt.year  # Extract year from fdate\n",
    "                \n",
    "                # Group data by year\n",
    "                for year, group in df.groupby('year'):\n",
    "                    if year is not pd.NaT and not pd.isna(year):\n",
    "                        if year not in annual_data:\n",
    "                            annual_data[year] = group.copy()\n",
    "                        else:\n",
    "                            annual_data[year] = pd.concat([annual_data[year], group], ignore_index=True)\n",
    "        \n",
    "        # Save annual aggregated data\n",
    "        for year, df in annual_data.items():\n",
    "            output_file = os.path.join(ANNUAL_FOLDER, f\"SEC_13F_Holdings_in_fdate_{year}.csv\")\n",
    "            df.drop(columns=['year'], inplace=True)\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"[INFO] Saved annual aggregated CSV for filling year {year}: {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed during annual aggregation: {e}\")\n",
    "\n",
    "# Step 8: Sort every csv file by rdate and cik in annual folder and base folder\n",
    "def sort_csv_files():\n",
    "    csv_files = [f for f in os.listdir(BASE_FOLDER) if f.endswith('.csv')]\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(BASE_FOLDER, file), low_memory=False)\n",
    "        df.sort_values(by=['rdate', 'cik'], inplace=True)\n",
    "        df.to_csv(os.path.join(BASE_FOLDER, file), index=False)\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(ANNUAL_FOLDER) if f.endswith('.csv')]\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(ANNUAL_FOLDER, file), low_memory=False)\n",
    "        df.sort_values(by=['rdate', 'cik'], inplace=True)\n",
    "        df.to_csv(os.path.join(ANNUAL_FOLDER, file), index=False)\n",
    "\n",
    "# Run the main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Combining all annual CSV files\n",
      "[INFO] Filtered 13F fillings saved as CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)\n",
    "\n",
    "# Combine all CSV files in the annual folder\n",
    "print(\"[INFO] Combining all annual CSV files\")\n",
    "all_files = glob.glob('sec_13f_annual/*.csv')\n",
    "df_list = [pd.read_csv(f, low_memory=False) for f in all_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "sorted_df = combined_df.sort_values(by='rdate')\n",
    "sorted_df.to_csv('Combined_sorted_13F.csv', index=False)\n",
    "# Filter 13F fillings and save as compressed file\n",
    "input_df = pd.read_csv('Combined_sorted_13F.csv')\n",
    "filtered_df = input_df[(input_df['rdate'] >= '2013-03-31') & (input_df['rdate'] <= '2023-12-31')]\n",
    "filtered_df.to_csv('13F_fillings.csv.gz', compression='gzip', index=False)\n",
    "os.rename('13F_fillings.csv.gz', '13F_fillings.gz')\n",
    "\n",
    "print(\"[INFO] Filtered 13F fillings saved as CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
